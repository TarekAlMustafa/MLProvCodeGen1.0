def set_train_log(log: str, value: str):
    log+= value
    return log

num_epochs = {{ num_epochs }}
epoch_log = ""
# Set up pytorch-ignite trainer and evaluator.
trainer = create_supervised_trainer(
    model,
    optimizer,
    loss_func,
    device=device,
)
{# TODO: Atm, the train metrics get accumulated, see torch_models.py #}
metrics = {
	"report": ClassificationReport(),
    "accuracy": Accuracy(),
    "loss": Loss(loss_func),
}
evaluator = create_supervised_evaluator(
    model, metrics=metrics, device=device
)

@trainer.on(Events.ITERATION_COMPLETED(every=print_every))
def log_batch(trainer):
    batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1
    print(
        f"Epoch {trainer.state.epoch} / {num_epochs}, "
        f"batch {batch} / {trainer.state.epoch_length}: "
        f"loss: {trainer.state.output:.3f}"
    )

@trainer.on(Events.EPOCH_COMPLETED)
def log_epoch(trainer):
    print(f"Epoch {trainer.state.epoch} / {num_epochs} average results: ")
    train_log = set_train_log('', (f"Epoch {trainer.state.epoch} / {num_epochs} average results: "))

    def log_results(name, metrics, epoch):
        print(
            f"{name + ':':6} loss: {metrics['loss']:.3f}, "
            f"accuracy: {metrics['accuracy']:.3f}"
        )
        
        log = set_train_log(train_log, (
            f"{name + ':':6} loss: {metrics['loss']:.3f}, "
            f"accuracy: {metrics['accuracy']:.3f}"
        ))
        return log

    # Train data.
    evaluator.run(train_loader)
    log_results("train", evaluator.state.metrics, trainer.state.epoch)
    
    # Val data.
    if val_loader:
        evaluator.run(val_loader)
        log_results("val", evaluator.state.metrics, trainer.state.epoch)

    # Test data.
    if test_loader:
        evaluator.run(test_loader)
        log_results("test", evaluator.state.metrics, trainer.state.epoch)

    print()
    print("-" * 80)
    print()
    return train_log

{# TODO: Maybe use this instead: https://pytorch.org/ignite/handlers.html#ignite.handlers.ModelCheckpoint #}
{% if checkpoint %}
@trainer.on(Events.EPOCH_COMPLETED)
def checkpoint_model(trainer):
    torch.save(model, checkpoint_dir / f"model-epoch{trainer.state.epoch}.pt")

{% endif %}
# Start training.
train_log = trainer.run(train_loader, max_epochs=num_epochs)

print(train_log)
{% if visualization_tool == "Weights & Biases" %}
wandb.finish()
{% endif %}

#set_training
d1.add_namespace('training', 'training.org')

e_training = d1.entity('training', (
    ('training:batch_size', batch_size),    ('training:epochs', num_epochs),
    ('training:train_metrics', str(train_log)),
    ('training:print_progress', 1),
))
a_settraining = d1.activity('set_training()', datetime.datetime.now())
d1.wasGeneratedBy(e_training, a_settraining, None, {'packages:fct': 'set'})
d1.used(a_settraining, e_modelparameters)
d1.used(a_settraining, e_datasegregation)