{%if checkpoint%}# Set up logging.
experiment_id = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
checkpoint_dir = Path(f"checkpoints/{experiment_id}")
checkpoint_dir.mkdir(parents=True, exist_ok=True){%endif%}
# Set up model, loss, optimizer.
lr = {{ lr }}
model = models.{{ model_func }}(pretrained={{ pretrained }})
{% if "resnet18" in model_func %}
model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=num_classes, bias=True)
{% elif "alexnet" in model_func or "vgg" in model_func %}
model.classifier[-1] = torch.nn.Linear(in_features=model.classifier[-1].in_features, out_features=num_classes, bias=True)
{% elif "densenet161" in model_func %}
model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=num_classes, bias=True)
{% endif %}
{% endif %}
model = model.to(device)
loss_func = nn.{{ loss }}()
optimizer = optim.{{ optimizer }}(model.parameters(), lr=lr)

{% if visualization_tool == "Weights & Biases" %}
# Log gradients and model parameters to W&B.
wandb.watch(model)

{% endif %}
#set_model_parameters
d1.add_namespace('modelparameters', 'modelparameters.org')

e_modelparameters = d1.entity('model_parameters', (
    ('modelparameters:model_name', 'resnet18'),
    ('modelparameters:save_checkpoint', 0),
    ('modelparameters:pretrained', 0),
    ('modelparameters:gpu_enable', 1),
    #('modelparameters:modelParameters', str(model)),
    ('modelparameters:loss_function', '{{ loss }}'),
    ('modelparameters:optimizer', '{{ optimizer }}'),
    ('modelparameters:optimizer_learning_rate', lr),
    ('modelparameters:num_classes', 1000),
))
a_setmodelparameters = d1.activity('set_model_parameters()', datetime.datetime.now())
d1.wasGeneratedBy(e_modelparameters, a_setmodelparameters, None, {'packages:fct': 'set'})
d1.used(a_setmodelparameters, e_dataingestion)

model