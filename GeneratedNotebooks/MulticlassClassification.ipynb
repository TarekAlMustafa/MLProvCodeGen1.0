{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14633fc",
   "metadata": {},
   "source": [
    "\n",
    "# Multiclass Classification\n",
    "\n",
    "Building a neural network to solve a multiclass classification exercise using the PyTorch framework.<br>\n",
    "Classification is one of the basic machine learning exercises. A trained model aims to predict the class of an input unit with high accuracy. <br>\n",
    "This neural network uses supervised learning, meaning that the input datasets also provide target labels to train the model with. <br>\n",
    "<br>\n",
    "This Notebook has been generated automatically using the JupyterLab extension ***MLProvCodeGen***.\n",
    "<br>\n",
    "\n",
    "Original Source Code and inspiration from this article https://janakiev.com/blog/pytorch-iris/ <br>\n",
    "Original author: N. Janakiev https://github.com/njanakiev Twitter: https://twitter.com/njanakiev\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79be5d3",
   "metadata": {},
   "source": [
    "### Installs\n",
    "Install required packages before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy===1.22.2 pandas===1.3.3 matplotlib===3.5.1 sklearn==0.0 torch===1.8.0 tqdm===4.60.0 ipywidgets===7.6.5 pytorch-model-summary===0.1.2 ipython===7.31.1 gputil===1.4.0 psutil===5.9.0 py-cpuinfo===8.0.0 --user\n",
    "#torch currently not supported with python 3.10, downgrading to python 3.9.7 possibly required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18fb84",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import GPUtil\n",
    "import psutil\n",
    "import cpuinfo\n",
    "import platform\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, f1_score, mean_absolute_error, mean_squared_error \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "from datetime import date\n",
    "import time\n",
    "from pytorch_model_summary import hierarchical_summary\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "import webbrowser\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48faf912",
   "metadata": {},
   "source": [
    "### Provenance Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(bytes, suffix=\"B\"):\n",
    "    \"\"\"\n",
    "    Scale bytes to its proper format\n",
    "    e.g:\n",
    "        1253656 => '1.20MB'\n",
    "        1253656678 => '1.17GB'\n",
    "    \"\"\"\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "\n",
    "def set_experiment_info() :\n",
    "    created_by = \"Tarek Al Mustafa\"\n",
    "    email = \"tarek.almustafa@uni-jena.de\"\n",
    "    title = \"Multiclass Classification\"\n",
    "    task_type = \"MulticlassClassification\"\n",
    "    creation_date = str(date.today())\n",
    "    \n",
    "    experiment_info = { \n",
    "            'author': created_by,\n",
    "            'email': email,\n",
    "            'title': title,\n",
    "            'creation_date': creation_date,\n",
    "            'task_type': task_type}\n",
    "    \n",
    "    return experiment_info\n",
    "\n",
    "def set_hardware_info():\n",
    "    uname = platform.uname()\n",
    "    sysInfo = str(uname.system +' '+ uname.release +' Version: '+ uname.version +' Machine: '+ uname.machine)\n",
    "    \n",
    "    svmem = psutil.virtual_memory()\n",
    "\n",
    "    GPUs = GPUtil.getGPUs()\n",
    "    gpuList = []\n",
    "    for gpu in GPUs:\n",
    "        gpu_id = gpu.id\n",
    "        gpu_name = gpu.name\n",
    "        gpuList.append((gpu_id , gpu_name))\n",
    "\n",
    "    hardware_info = {\n",
    "        \"Python Version\": cpuinfo.get_cpu_info()['python_version'],\n",
    "        \"CPU\": cpuinfo.get_cpu_info()['brand_raw'],\n",
    "        \"RAM\": get_size(svmem.total),\n",
    "        \"Operating System\": sysInfo,\n",
    "        \"GPUs\": str(gpuList) }\n",
    "    \n",
    "    return hardware_info\n",
    "\n",
    "def set_packages():\n",
    "    cpuInfo_version = !pip list | grep -i py-cpuinfo\n",
    "    pytorch_model_summary_version = !pip list | grep -i pytorch-model-summary\n",
    "    packages = {\n",
    "        \"numpy\" : np.__version__,\n",
    "        \"pandas\" : pd.__version__,\n",
    "        \"matplotlib\" : matplotlib.__version__,\n",
    "        \"sklearn\" : sklearn.__version__,\n",
    "        \"torch\" : torch.__version__,\n",
    "        \"tqdm\" : tqdm.__version__,\n",
    "        \"ipywidgets\" : widgets.__version__,\n",
    "        \"pytorch-model-summary\" : pytorch_model_summary_version[0],\n",
    "        \"ipython\" : IPython.__version__,\n",
    "        \"gputil\" : GPUtil.__version__, \n",
    "        \"psutil\" : psutil.__version__,\n",
    "        \"py-cpuinfo\" : cpuInfo_version[0]}\n",
    "    \n",
    "    return packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d5f91",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca96646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Ingestion\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "output_dim = len(names)\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "def set_data_ingestion():\n",
    "    data_ingestion = {\n",
    "        \"dataset_id\" : 'Iris',\n",
    "        \"output_dimensions\" : output_dim,\n",
    "        \"samples_total\" : len(X),\n",
    "        \"feature_dimensions\" : len(feature_names) }\n",
    "    \n",
    "    return data_ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd15f7",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c439fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preperation\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "def set_data_preparation():\n",
    "    data_preparation = {\n",
    "        \"number_operations\" : 1,\n",
    "        \"transformation\" : \"sklearn.preprocessing.StandardScaler\",\n",
    "        \"transformation_method\" : \"Standardscaler.fit_transform\"}\n",
    "    \n",
    "    return data_preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa0900",
   "metadata": {},
   "source": [
    "### Data Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae19402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Segregation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=2)\n",
    "\n",
    "def set_data_segregation():\n",
    "    data_segregation = {\n",
    "        \"segregation_method\" : \"sklearn.model_selection.train_test_split\",\n",
    "        \"test_size\" : 0.2,\n",
    "        \"train_size\" : 1-0.2,\n",
    "        \"random_state\" : 2 }\n",
    "    \n",
    "    return data_segregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c674487",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    ax1.plot(X_plot[:, 0], X_plot[:, 1], \n",
    "             linestyle='none', \n",
    "             marker='o', \n",
    "             label=target_name)\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.axis('equal')\n",
    "ax1.legend();\n",
    "\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    ax2.plot(X_plot[:, 2], X_plot[:, 3], \n",
    "             linestyle='none', \n",
    "             marker='o', \n",
    "             label=target_name)\n",
    "ax2.set_xlabel(feature_names[2])\n",
    "ax2.set_ylabel(feature_names[3])\n",
    "ax2.axis('equal')\n",
    "ax2.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c90c5",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f560d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use GPU?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "neuron_number = 50\n",
    "\n",
    "#Configure Neural Network Models\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 50)\n",
    "        self.layer3 = nn.Linear(50, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model     = Model(X_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "\n",
    "try:\n",
    "    lr\n",
    "except NameError:\n",
    "    default = 1\n",
    "    lr = None \n",
    "else:\n",
    "    default = 0\n",
    "\n",
    "def set_model_parameters():\n",
    "    model_parameters = {\n",
    "        \"gpu_enable\" : 1,\n",
    "        \"modelParameters\" : str(model),\n",
    "        \"neuron_number\" : neuron_number,\n",
    "        \"loss_function\" : 'nn.CrossEntropyLoss()',\n",
    "        \"optimizer\" : 'torch.optim.Adam(',\n",
    "        \"optimizer_default_learning_rate\" : default,\n",
    "        \"optimizer_learning_rate\": lr,\n",
    "        \"activation_function\": 'F.softmax(self.layer3(x), dim=1)' }\n",
    "    return model_parameters\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9122953",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47941e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training\n",
    "EPOCHS  = 100\n",
    "X_train = Variable(torch.from_numpy(X_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
    "\n",
    "loss_list     = np.zeros((EPOCHS,))\n",
    "accuracy_list = np.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss_list[epoch] = loss.item()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] = correct.mean()\n",
    "\n",
    "def set_training():\n",
    "    training = {\n",
    "    \"epochs\" : EPOCHS,\n",
    "    \"hierarchical_summary\" : hierarchical_summary(model, print_summary = False) }\n",
    "    \n",
    "    return training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c3847",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f370f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracy and Loss from Training\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(accuracy_list)\n",
    "ax1.set_ylabel(\"validation accuracy\")\n",
    "ax2.plot(loss_list)\n",
    "ax2.set_ylabel(\"validation loss\")\n",
    "ax2.set_xlabel(\"epochs\");\n",
    "\n",
    "def set_evaluation():\n",
    "    evaluation = {\n",
    "    \"Accuracy\": accuracy_list[(len(accuracy_list)-1)],\n",
    "    \"Loss\": loss_list[(len(loss_list)-1)],\n",
    "    \"Confusion Matrix\": str(confusionMatrix),\n",
    "    \"AUC\": float(AUC),\n",
    "    \"F1 Score\": str(F1),\n",
    "    \"Mean Absolute Error\": MAE,\n",
    "    \"Mean Squared Error\": MSE}\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3bedf9",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = confusion_matrix(y_test, torch.argmax(y_pred, dim=1))\n",
    "confusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ce28a",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adee00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = f1_score(y_test, torch.argmax(y_pred, dim=1), average=None)\n",
    "F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ad6e1",
   "metadata": {},
   "source": [
    "### Mean Absolute Error & Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, torch.argmax(y_pred, dim=1), multioutput='uniform_average')\n",
    "MSE = mean_squared_error(y_test, torch.argmax(y_pred, dim=1), multioutput='uniform_average')\n",
    "print(MAE)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0c3cb",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y_onehot = enc.fit_transform(y_test[:, np.newaxis]).toarray()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()\n",
    "    fpr, tpr, threshold = roc_curve(Y_onehot.ravel(), y_pred.ravel())\n",
    "    \n",
    "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc(fpr, tpr)))\n",
    "AUC = '{:.5f}'.format(auc(fpr, tpr))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006314b",
   "metadata": {},
   "source": [
    "### Generate Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac64cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_experiment_info()\n",
    "set_hardware_info()\n",
    "set_packages()\n",
    "set_data_ingestion()\n",
    "set_data_preparation()\n",
    "set_data_segregation()\n",
    "set_model_parameters()\n",
    "set_training()\n",
    "set_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cd0ae",
   "metadata": {},
   "source": [
    "### Write Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestring = time.strftime('%Y%m%d-%H%M%S')\n",
    "timestring\n",
    "ProvenanceName = ('Provenance_MulticlassClassification_' + timestring + '.json')\n",
    "\n",
    "with open('../GeneratedProvenanceData/'+ProvenanceName, 'w') as prov_file:\n",
    "    prov_file.write('{' + '\\n  ')\n",
    "    prov_file.write('\"experiment_info\":' + json.dumps(set_experiment_info(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"hardware_info\":' + json.dumps(set_hardware_info(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"packages\":' + json.dumps(set_packages(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_ingestion\":' + json.dumps(set_data_ingestion(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_preparation\":' + json.dumps(set_data_preparation(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_segregation\":' + json.dumps(set_data_segregation(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"model_parameters\":' + json.dumps(set_model_parameters(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"training\":' + json.dumps(set_training(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"evaluation\":' + json.dumps(set_evaluation(),sort_keys=False, indent=4) + '\\n' )\n",
    "    prov_file.write('}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f03d3",
   "metadata": {},
   "source": [
    "### Show Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1146165",
   "metadata": {},
   "outputs": [],
   "source": [
    "provenance_open = widgets.Button(description = 'Open Provenance Data File')\n",
    "display(provenance_open)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    provenance_open.on_click = webbrowser.open('http://localhost:8888/lab/tree/GeneratedProvenanceData/'+ProvenanceName)\n",
    "\n",
    "provenance_open.on_click(on_button_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
