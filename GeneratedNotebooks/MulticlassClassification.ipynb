{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc4873f",
   "metadata": {},
   "source": [
    "\n",
    "# Multiclass Classification\n",
    "\n",
    "Building a neural network to solve a multiclass classification exercise using the PyTorch framework.<br>\n",
    "Classification is one of the basic machine learning exercises. A trained model aims to predict the class of an input unit with high accuracy. <br>\n",
    "This neural network uses supervised learning, meaning that the input datasets also provide target labels to train the model with. <br>\n",
    "<br>\n",
    "This Notebook has been generated automatically using the JupyterLab extension ***MLProvCodeGen***.\n",
    "<br>\n",
    "\n",
    "Original Source Code and inspiration from this article https://janakiev.com/blog/pytorch-iris/ <br>\n",
    "Original author: N. Janakiev https://github.com/njanakiev Twitter: https://twitter.com/njanakiev\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c5341",
   "metadata": {},
   "source": [
    "### Installs\n",
    "Install required packages before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy===1.22.2 pandas===1.3.3 matplotlib===3.5.1 sklearn==0.0 torch===1.8.0 tqdm===4.60.0 ipywidgets===7.6.5 pytorch-model-summary===0.1.2 ipython===7.31.1 --user\n",
    "#torch currently not supported with python 3.10, downgrading to python 3.9.7 possibly required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a09ce",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d19c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "from datetime import date\n",
    "import time\n",
    "from pytorch_model_summary import hierarchical_summary\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "import webbrowser\n",
    "from IPython.display import display\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3ce33",
   "metadata": {},
   "source": [
    "### Provenance Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_experiment_info() : \n",
    "    created_by = \"Tarek Al Mustafa\"\n",
    "    email = \"tarek.almustafa@uni-jena.de\"\n",
    "    title = \"Multiclass Classification\"\n",
    "    creation_date = str(date.today())\n",
    "    \n",
    "    experiment_info = { \n",
    "            'author': created_by,\n",
    "            'email': email,\n",
    "            'title': title,\n",
    "            'creation_date': creation_date }\n",
    "    \n",
    "    return experiment_info\n",
    "\n",
    "def set_hardware_info():\n",
    "    hardware_info = {\n",
    "        \"cpu\": \"Intel Core i7\",\n",
    "        \"memory\": \"8 GB\",\n",
    "        \"hdType\": \"SSD\",\n",
    "        \"cache\": '6M',\n",
    "        \"os\": \"OSX El Capitan 10.11.5\",\n",
    "        \"video\": \"Nvidia GForce 630M\" }\n",
    "    \n",
    "    return hardware_info\n",
    "\n",
    "def set_packages():\n",
    "    packages = {\n",
    "        \"python\" : \"python===3.9.7\",\n",
    "        \"numpy\" : \"numpy===1.22.2\",\n",
    "        \"pandas\" : \"pandas===1.3.3\",\n",
    "        \"matplotlib\" : \"matplotlib===3.5.1\",\n",
    "        \"sklearn\" : \"sklearn==0.0\",\n",
    "        \"torch\" : \"torch===1.8.0\",\n",
    "        \"tqdm\" : \"tqdm===4.60.0\",\n",
    "        \"ipywidgets\" : \"ipywidgets===7.6.5\",\n",
    "        \"pytorch_model_summary\" : \"pytorch-model-summary===0.1.2\",\n",
    "        \"ipython\" : \"ipython===7.31.1\"}\n",
    "    \n",
    "    return packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8222cbb",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Ingestion\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "output_dim = len(names)\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "def set_data_ingestion():\n",
    "    data_ingestion = {\n",
    "        \"dataset_id\" : 'Iris',\n",
    "        \"output_dimensions\" : output_dim,\n",
    "        \"samples_total\" : len(X),\n",
    "        \"feature_dimensions\" : len(feature_names) }\n",
    "    \n",
    "    return data_ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece114c5",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50526890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preperation\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "def set_data_preparation():\n",
    "    data_preparation = {\n",
    "        \"number_operations\" : 1,\n",
    "        \"transformation\" : \"sklearn.preprocessing.StandardScaler\",\n",
    "        \"transformation_method\" : \"Standardscaler.fit_transform\"}\n",
    "    \n",
    "    return data_preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7ce17",
   "metadata": {},
   "source": [
    "### Data Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Segregation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=2)\n",
    "\t\n",
    "def set_data_segregation():\n",
    "    data_segregation = {\n",
    "        \"segregation_method\" : \"sklearn.model_selection.train_test_split\",\n",
    "        \"test_size\" : 0.2,\n",
    "        \"train_size\" : 1-0.2,\n",
    "        \"random_state\" : 2 }\n",
    "    \n",
    "    return data_segregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84814c",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f091e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    ax1.plot(X_plot[:, 0], X_plot[:, 1], \n",
    "             linestyle='none', \n",
    "             marker='o', \n",
    "             label=target_name)\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.axis('equal')\n",
    "ax1.legend();\n",
    "\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    ax2.plot(X_plot[:, 2], X_plot[:, 3], \n",
    "             linestyle='none', \n",
    "             marker='o', \n",
    "             label=target_name)\n",
    "ax2.set_xlabel(feature_names[2])\n",
    "ax2.set_ylabel(feature_names[3])\n",
    "ax2.axis('equal')\n",
    "ax2.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc809953",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use GPU?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "#Configure Neural Network Models\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 50)\n",
    "        self.layer3 = nn.Linear(50, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model     = Model(X_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_model_parameters():\n",
    "    model_parameters = {\n",
    "        \"gpu_enable\" : 1,\n",
    "        \"modelParameters\" : str(model),\n",
    "        \"loss_function\" : 'nn.CrossEntropyLoss()',\n",
    "        \"optimizer\" : str(optimizer),\n",
    "        \"activation_function\": 'F.softmax(self.layer3(x), dim=1)' }\n",
    "    return model_parameters\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba719b1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd29532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training\n",
    "EPOCHS  = 100\n",
    "X_train = Variable(torch.from_numpy(X_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
    "\n",
    "loss_list     = np.zeros((EPOCHS,))\n",
    "accuracy_list = np.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss_list[epoch] = loss.item()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] = correct.mean()\n",
    "\t\t\n",
    "def set_training():\n",
    "    training = {\n",
    "    \"epochs\" : EPOCHS,\n",
    "\t\"hierarchical_summary\" : hierarchical_summary(model, print_summary = False) }\n",
    "    \n",
    "    return training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16468168",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d065297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracy and Loss from Training\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(accuracy_list)\n",
    "ax1.set_ylabel(\"validation accuracy\")\n",
    "ax2.plot(loss_list)\n",
    "ax2.set_ylabel(\"validation loss\")\n",
    "ax2.set_xlabel(\"epochs\");\n",
    "\n",
    "def set_evaluation():\n",
    "    evaluation = {\n",
    "    \"accuracy\": accuracy_list[(len(accuracy_list)-1)],\n",
    "    \"loss\": loss_list[(len(loss_list)-1)]}\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e02730",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e50c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y_onehot = enc.fit_transform(y_test[:, np.newaxis]).toarray()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()\n",
    "    fpr, tpr, threshold = roc_curve(Y_onehot.ravel(), y_pred.ravel())\n",
    "    \n",
    "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc(fpr, tpr)))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72406038",
   "metadata": {},
   "source": [
    "### Generate Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_experiment_info()\n",
    "set_hardware_info()\n",
    "set_packages()\n",
    "set_data_ingestion()\n",
    "set_data_preparation()\n",
    "set_data_segregation()\n",
    "set_model_parameters()\n",
    "set_training()\n",
    "set_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519f816",
   "metadata": {},
   "source": [
    "### Write Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestring = time.strftime('%Y%m%d-%H%M%S')\n",
    "timestring\n",
    "ProvenanceName = ('Provenance_MulticlassClassification_' + timestring + '.json')\n",
    "\n",
    "with open('../GeneratedProvenanceData/'+ProvenanceName, 'w') as prov_file:\n",
    "    prov_file.write('{' + '\\n  ')\n",
    "    prov_file.write('\"experiment info\":' + json.dumps(set_experiment_info(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"hardware info\":' + json.dumps(set_hardware_info(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"packages\":' + json.dumps(set_packages(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_ingestion\":' + json.dumps(set_data_ingestion(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_preparation\":' + json.dumps(set_data_preparation(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_segregation\":' + json.dumps(set_data_segregation(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"model_parameters\":' + json.dumps(set_model_parameters(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"training\":' + json.dumps(set_training(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"evaluation\":' + json.dumps(set_evaluation(),sort_keys=False, indent=4) + '\\n' )\n",
    "    prov_file.write('}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06063695",
   "metadata": {},
   "source": [
    "### Show Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdeb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "provenance_open = widgets.Button(description = 'Open Provenance Data File')\n",
    "display(provenance_open)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    provenance_open.on_click = webbrowser.open('http://localhost:8888/lab/tree/GeneratedProvenanceData/'+ProvenanceName)\n",
    "\n",
    "provenance_open.on_click(on_button_clicked)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
