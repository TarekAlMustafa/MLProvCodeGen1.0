{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361b79fd",
   "metadata": {},
   "source": [
    "\n",
    "# Image Classification\n",
    "\n",
    "Building a machine learning model to solve Image Classification using the PyTorch framework.<br>\n",
    "Image Classification is one of the basic pattern recognition exercises. <br>\n",
    "Using Image files as its input, a model trained for Image classification will split a set of images into a given number of classes. <br>\n",
    "<br>\n",
    "This Notebook has been generated automatically using the JupyterLab extension ***MLProvCodeGen***.\n",
    "<br>\n",
    "The original Source Code is from this application https://github.com/jrieke/traingenerator <br>\n",
    "Made by: https://www.jrieke.com/ Twitter: https://twitter.com/jrieke\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23b216",
   "metadata": {},
   "source": [
    "### Installs\n",
    "Install required packages before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa58afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy===1.22.2 ipywidgets===7.6.5 torch===1.10.2 torchvision===0.11.3 pytorch-ignite===0.4.6 gputil===1.4.0 psutil===5.9.0 py-cpuinfo===8.0.0 --user\n",
    "#torch currently not supported with python 3.10, downgrading to python 3.9.7 possibly required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92d262",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e60cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision as torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "import ignite as pytorch_ignite\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "import GPUtil\n",
    "import psutil\n",
    "import cpuinfo\n",
    "import platform\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7095d",
   "metadata": {},
   "source": [
    "### Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92b731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Tarek Al Mustafa', 'email': 'tarek.almustafa@uni-jena.de', 'title': 'Image Classification', 'creation_date': '2022-03-23', 'task_type': 'Image Classification Pytorch'}\n",
      "{'Python Version': '3.9.7.final.0 (64 bit)', 'CPU': 'AMD Ryzen 7 3700X 8-Core Processor', 'RAM': '15.95GB', 'Operating System': 'Windows 10 Version: 10.0.19041 Machine: AMD64', 'GPUs': \"[(0, 'NVIDIA GeForce GTX 1060 6GB')]\"}\n",
      "{'numpy': '1.22.2', 'ipywidgets': '7.6.5', 'torch': '1.10.2+cpu', 'torchvision': '0.11.3+cpu', 'pytorch-ignite': '0.4.6', 'gputil': '1.4.0', 'psutil': '5.9.0', 'py-cpuinfo': 'py-cpuinfo                    8.0.0'}\n"
     ]
    }
   ],
   "source": [
    "def get_size(bytes, suffix=\"B\"):\n",
    "    \"\"\"\n",
    "    Scale bytes to its proper format\n",
    "    e.g:\n",
    "        1253656 => '1.20MB'\n",
    "        1253656678 => '1.17GB'\n",
    "    \"\"\"\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "\n",
    "def set_experiment_info() :\n",
    "    created_by = \"Tarek Al Mustafa\"\n",
    "    email = \"tarek.almustafa@uni-jena.de\"\n",
    "    title = \"Image Classification\"\n",
    "    task_type = \"Image Classification Pytorch\"\n",
    "    creation_date = str(date.today())\n",
    "    \n",
    "    experiment_info = { \n",
    "            'author': created_by,\n",
    "            'email': email,\n",
    "            'title': title,\n",
    "            'creation_date': creation_date,\n",
    "            'task_type': task_type}\n",
    "    \n",
    "    return experiment_info\n",
    "\n",
    "def set_hardware_info():\n",
    "    uname = platform.uname()\n",
    "    sysInfo = str(uname.system +' '+ uname.release +' Version: '+ uname.version +' Machine: '+ uname.machine)\n",
    "    \n",
    "    svmem = psutil.virtual_memory()\n",
    "\n",
    "    GPUs = GPUtil.getGPUs()\n",
    "    gpuList = []\n",
    "    for gpu in GPUs:\n",
    "        gpu_id = gpu.id\n",
    "        gpu_name = gpu.name\n",
    "        gpuList.append((gpu_id , gpu_name))\n",
    "\n",
    "    hardware_info = {\n",
    "        \"Python Version\": cpuinfo.get_cpu_info()['python_version'],\n",
    "        \"CPU\": cpuinfo.get_cpu_info()['brand_raw'],\n",
    "        \"RAM\": get_size(svmem.total),\n",
    "        \"Operating System\": sysInfo,\n",
    "        \"GPUs\": str(gpuList) }\n",
    "    \n",
    "    return hardware_info\n",
    "\n",
    "def set_packages():\n",
    "    cpuInfo_version = !pip list | grep -i py-cpuinfo\n",
    "    pytorch_model_summary_version = !pip list | grep -i pytorch-model-summary\n",
    "    packages = {\n",
    "        \"numpy\" : np.__version__,\n",
    "        \"ipywidgets\" : widgets.__version__,\n",
    "        \"torch\" : torch.__version__,\n",
    "        \"torchvision\" : torchvision.__version__,\n",
    "        \"pytorch-ignite\" : pytorch_ignite.__version__,\n",
    "        \"gputil\" : GPUtil.__version__, \n",
    "        \"psutil\" : psutil.__version__,\n",
    "        \"py-cpuinfo\" : cpuInfo_version[0]}\n",
    "\n",
    "    return packages\n",
    "\n",
    "print(set_experiment_info())\n",
    "print(set_hardware_info())\n",
    "print(set_packages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a8a40",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d22619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset  will be loaded further down.\n",
    "\n",
    "\n",
    "\n",
    "# Set up logging.\n",
    "print_every = 1  # batches\n",
    "\n",
    "# Set up device.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b996d",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f82fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = getattr(datasets, 'FakeData')\n",
    "\n",
    "def set_data_ingestion():\n",
    "    dataInfo = training_dataset.__len__\n",
    "    root_location =  str(dataInfo).splitlines()[2]\n",
    "    data_ingestion = {\n",
    "        \"dataset_id\" : 'MNIST',\n",
    "        \"feature_classes\" : 10,\n",
    "        \"training_samples\" : training_dataset.__len__(),\n",
    "        \"testing_samples\" : testing_dataset.__len__(),\n",
    "        \"root_location\" : root_location}\n",
    "    \n",
    "    return data_ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e19dfb",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e5c85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256), \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # grayscale to RGB\n",
    "])\n",
    "\n",
    "def set_data_preparation():\n",
    "    transform_method = str(dataInfo).splitlines()[4:11]\n",
    "    data_preparation = {\n",
    "        \"preprocessing\" : transform_method}\n",
    "    \n",
    "    return data_preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087b814",
   "metadata": {},
   "source": [
    "### Data Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59bd3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform2 = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# Wrap in data loader.\n",
    "training_dataset = dataset(transform = transform2)\n",
    "#training_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "testing_dataset = dataset(transform = transform2)\n",
    "\n",
    "if use_cuda:\n",
    "    kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
    "else:\n",
    "    kwargs = {}\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "val_loader = None\n",
    "\n",
    "def set_data_segregation():\n",
    "    data_segregation = {\n",
    "        \"training_dataset\" : str(training_dataset.__len__),\n",
    "        \"testing_dataset\" : str(testing_dataset.__len__),\n",
    "        \"preprocessing\" : transform_method}\n",
    "    \n",
    "    return data_segregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ccffb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e819dc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up model, loss, optimizer.\n",
    "lr = 0.001\n",
    "model = models.resnet18(pretrained=0)\n",
    "num_classes = 1000\n",
    "model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=num_classes, bias=True)\n",
    "model = model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def set_model_parameters():\n",
    "    model_parameters = {\n",
    "\t\t\"model_name\" : resnet18,\n",
    "\t\t\"pretrained\" : 0,\n",
    "        \"gpu_enable\" : 1,\n",
    "        \"modelParameters\" : str(model),\n",
    "        \"loss_function\" : str(CrossEntropyLoss),\n",
    "        \"optimizer\" : 'Adam',\n",
    "        \"optimizer_learning_rate\": lr}\n",
    "    return model_parameters\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52966afb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09fb81e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1, batch 1 / 8: loss: 7.139\n",
      "Epoch 1 / 1, batch 2 / 8: loss: 6.156\n",
      "Epoch 1 / 1, batch 3 / 8: loss: 5.230\n",
      "Epoch 1 / 1, batch 4 / 8: loss: 4.295\n",
      "Epoch 1 / 1, batch 5 / 8: loss: 3.441\n",
      "Epoch 1 / 1, batch 6 / 8: loss: 2.864\n",
      "Epoch 1 / 1, batch 7 / 8: loss: 2.599\n",
      "Epoch 1 / 1, batch 8 / 8: loss: 2.481\n",
      "Epoch 1 / 1 average results: \n",
      "train: loss: 4.423, accuracy: 0.093\n",
      "test:  loss: 4.423, accuracy: 0.093\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "State:\n",
      "\titeration: 8\n",
      "\tepoch: 1\n",
      "\tepoch_length: 8\n",
      "\tmax_epochs: 1\n",
      "\toutput: 2.4807040691375732\n",
      "\tbatch: <class 'list'>\n",
      "\tmetrics: <class 'dict'>\n",
      "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "\tseed: <class 'NoneType'>\n",
      "\ttimes: <class 'dict'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up hyperparameters.\n",
    "num_epochs = 1\n",
    "def set_train_log(log: str, value: str):\n",
    "    log+= value\n",
    "    return log\n",
    "    \n",
    "# Set up pytorch-ignite trainer and evaluator.\n",
    "trainer = create_supervised_trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    device=device,\n",
    ")\n",
    "metrics = {\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"loss\": Loss(loss_func),\n",
    "}\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model, metrics=metrics, device=device\n",
    ")\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=print_every))\n",
    "def log_batch(trainer):\n",
    "    batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1\n",
    "    print(\n",
    "        f\"Epoch {trainer.state.epoch} / {num_epochs}, \"\n",
    "        f\"batch {batch} / {trainer.state.epoch_length}: \"\n",
    "        f\"loss: {trainer.state.output:.3f}\"\n",
    "    )\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_epoch(trainer):\n",
    "    print(f\"Epoch {trainer.state.epoch} / {num_epochs} average results: \")\n",
    "    train_log = set_train_log('', (f\"Epoch {trainer.state.epoch} / {num_epochs} average results: \"))\n",
    "    \n",
    "    def log_results(name, metrics, epoch):\n",
    "        print(\n",
    "            f\"{name + ':':6} loss: {metrics['loss']:.3f}, \"\n",
    "            f\"accuracy: {metrics['accuracy']:.3f}\"\n",
    "        )\n",
    "        log = set_train_log(train_log, (\n",
    "            f\"{name + ':':6} loss: {metrics['loss']:.3f}, \"\n",
    "            f\"accuracy: {metrics['accuracy']:.3f}\"\n",
    "        ))\n",
    "        return log\n",
    "\n",
    "    # Train data.\n",
    "    evaluator.run(train_loader)\n",
    "    log_results(\"train\", evaluator.state.metrics, trainer.state.epoch)\n",
    "    \n",
    "    # Val data.\n",
    "    if val_loader:\n",
    "        evaluator.run(val_loader)\n",
    "        log_results(\"val\", evaluator.state.metrics, trainer.state.epoch)\n",
    "\n",
    "    # Test data.\n",
    "    if test_loader:\n",
    "        evaluator.run(test_loader)\n",
    "        log_results(\"test\", evaluator.state.metrics, trainer.state.epoch)\n",
    "\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    return train_log\n",
    "\n",
    "# Start training.\n",
    "train_log = trainer.run(train_loader, max_epochs=num_epochs)\n",
    "\n",
    "print(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379bba5",
   "metadata": {},
   "source": [
    "### Generate Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a966e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:\n",
      "\titeration: 8\n",
      "\tepoch: 1\n",
      "\tepoch_length: 8\n",
      "\tmax_epochs: 1\n",
      "\toutput: 2.4807040691375732\n",
      "\tbatch: <class 'list'>\n",
      "\tmetrics: <class 'dict'>\n",
      "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "\tseed: <class 'NoneType'>\n",
      "\ttimes: <class 'dict'>\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataInfo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7048/632714581.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mset_packages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mset_data_ingestion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mset_data_preparation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mset_data_segregation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mset_model_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7048/2602057083.py\u001b[0m in \u001b[0;36mset_data_preparation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mset_data_preparation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtransform_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataInfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     data_preparation = {\n\u001b[0;32m     11\u001b[0m         \"preprocessing\" : transform_method}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataInfo' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_log)\n",
    "set_experiment_info()\n",
    "set_hardware_info()\n",
    "set_packages()\n",
    "set_data_ingestion()\n",
    "set_data_preparation()\n",
    "set_data_segregation()\n",
    "set_model_parameters()\n",
    "set_training()\n",
    "set_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da50488",
   "metadata": {},
   "source": [
    "### Write Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestring = time.strftime('%Y%m%d-%H%M%S')\n",
    "timestring\n",
    "ProvenanceName = ('Provenance_MulticlassClassification_' + timestring + '.json')\n",
    "\n",
    "with open('../GeneratedProvenanceData/'+ProvenanceName, 'w') as prov_file:\n",
    "    prov_file.write('{' + '\\n  ')\n",
    "    prov_file.write('\"experiment_info\":' + json.dumps(set_experiment_info(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"hardware_info\":' + json.dumps(set_hardware_info(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"packages\":' + json.dumps(set_packages(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_ingestion\":' + json.dumps(set_data_ingestion(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_preparation\":' + json.dumps(set_data_preparation(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"data_segregation\":' + json.dumps(set_data_segregation(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"model_parameters\":' + json.dumps(set_model_parameters(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"training\":' + json.dumps(set_training(),sort_keys=False, indent=4) +',' + '\\n\\n' )\n",
    "    prov_file.write('\"evaluation\":' + json.dumps(set_evaluation(),sort_keys=False, indent=4) + '\\n' )\n",
    "    prov_file.write('}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec90a13",
   "metadata": {},
   "source": [
    "### Open Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e93281",
   "metadata": {},
   "outputs": [],
   "source": [
    "provenance_open = widgets.Button(description = 'Open Provenance Data File')\n",
    "display(provenance_open)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    provenance_open.on_click = webbrowser.open('http://localhost:8888/lab/tree/GeneratedProvenanceData/'+ProvenanceName)\n",
    "\n",
    "provenance_open.on_click(on_button_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
