{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1aa15d9",
   "metadata": {},
   "source": [
    "\n",
    "# Image Classification\n",
    "\n",
    "Building a machine learning model to solve Image Classification using the PyTorch framework.<br>\n",
    "Image Classification is one of the basic pattern recognition exercises. <br>\n",
    "Using Image files as its input, a model trained for Image classification will split a set of images into a given number of classes. <br>\n",
    "<br>\n",
    "This Notebook has been generated automatically using the JupyterLab extension ***MLProvCodeGen***.\n",
    "<br>\n",
    "The original Source Code is from this application https://github.com/jrieke/traingenerator <br>\n",
    "Made by: https://www.jrieke.com/ Twitter: https://twitter.com/jrieke\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510d07e",
   "metadata": {},
   "source": [
    "### Installs\n",
    "Install required packages before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52a16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy===1.22.2 ipywidgets===7.6.5 torch===1.10.2 torchvision===0.11.3 pytorch-ignite===0.4.6 pytorch-lightning===1.5.10 gputil===1.4.0 psutil===5.9.0 py-cpuinfo===8.0.0 prov===2.0.0 pydot===1.4.2 --user\n",
    "#torch currently not supported with python 3.10, downgrading to python 3.9.7 possibly required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09355f83",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fcccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision as torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "import ignite as pytorch_ignite\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, ClassificationReport\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning import seed_everything\n",
    "import GPUtil\n",
    "import psutil\n",
    "import cpuinfo\n",
    "import platform\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "import json\n",
    "import webbrowser\n",
    "import IPython\n",
    "from IPython.display import display, Image\n",
    "import prov\n",
    "from prov.model import ProvDocument\n",
    "from prov.dot import prov_to_dot\n",
    "import os\n",
    "from prov.model import ProvDocument, Namespace, Literal, PROV, Identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64870661",
   "metadata": {},
   "source": [
    "### Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c1b8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ProvGeneration: (ex:Packages Data, ex:set_packages())>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_size(bytes, suffix=\"B\"):\n",
    "    \"\"\"\n",
    "    Scale bytes to its proper format\n",
    "    e.g:\n",
    "        1253656 => '1.20MB'\n",
    "        1253656678 => '1.17GB'\n",
    "    \"\"\"\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "\n",
    "d1 = ProvDocument()\n",
    "d1.add_namespace('prov', 'http://www.w3.org/ns/prov#')\n",
    "d1.add_namespace('ex', 'https://github.com/TarekAlMustafa/MLProvCodeGen1.0/')\n",
    "d1.add_namespace('foaf', 'http://xmlns.com/foaf/0.1/')\n",
    "d1.add_namespace('p-plan', 'http://purl.org/net/p-plan')\n",
    "\n",
    "e_MLProvCodeGen = d1.entity(\n",
    "        'ex:MLProvCodeGen',(\n",
    "            ('prov:type', PROV['Plan']),\n",
    "))\n",
    "ag_author = d1.agent(\n",
    "        'ex:Tarek Al Mustafa',(\n",
    "            ('prov:type', PROV['Person']),\n",
    "            ('foaf:givenName', 'Tarek Al Mustafa'),\n",
    "            ('foaf:mbox', '<tarek.almustafa@uni-jena.de>'),\n",
    "            ('prov:role', 'Author'),\n",
    "))\n",
    "kernellist = !jupyter kernelspec list\n",
    "e_notebook = d1.entity(\n",
    "        'ex:notebook',(\n",
    "            ('ex:programming_language','Python'),\n",
    "            ('ex:programming_language_version', cpuinfo.get_cpu_info()['python_version']),\n",
    "            ('ex:kernel','python3(ipykernel)'),\n",
    "            ('prov:type', PROV['File']),\n",
    "            ('ex:fileformat', '.ipynb'),\n",
    "            ('ex:name', 'MulticlassClassification.ipynb'),\n",
    "            ('ex:creation_date', str(date.today())),\n",
    "            ('ex:last_modified', 'TODO'),\n",
    "))\n",
    "e_notebook.add_asserted_type('prov:Collection')\n",
    "d1.wasAttributedTo(e_notebook, ag_author)\n",
    "a_generateNotebook = d1.activity('ex:GenerateNotebook')\n",
    "d1.wasAssociatedWith(a_generateNotebook, ag_author, plan=e_MLProvCodeGen)\n",
    "d1.wasGeneratedBy(e_notebook, a_generateNotebook)\n",
    "\n",
    "#set_experimentinfo\n",
    "e_experimentinfo = d1.entity('ex:Cell Experiment Info', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setexperimentinfo = d1.activity('ex:set_experiment_info()')\n",
    "a_setdate = d1.activity('ex:date.today()')\n",
    "\n",
    "d1.wasStartedBy(a_setexperimentinfo,e_experimentinfo, time=datetime.datetime.now())\n",
    "d1.wasInformedBy(a_setexperimentinfo, a_setdate)  \n",
    "d1.hadMember(e_notebook, e_experimentinfo)\n",
    "e_experimentinfo_data = d1.entity(\n",
    "    'ex:Experiment Info Data',(\n",
    "        ('ex:title', 'Image Classification'),\n",
    "        ('ex:creation_date', str(date.today())),\n",
    "        ('ex:task_type', 'ImageClassification_pytorch'),\n",
    "))\n",
    "d1.wasGeneratedBy(e_experimentinfo_data, a_setexperimentinfo)\n",
    "\n",
    "#set_hardware_info()\n",
    "uname = platform.uname()\n",
    "sysInfo = str(uname.system +' '+ uname.release +' Version: '+ uname.version +' Machine: '+ uname.machine)\n",
    "    \n",
    "svmem = psutil.virtual_memory()\n",
    "\n",
    "GPUs = GPUtil.getGPUs()\n",
    "gpuList = []\n",
    "for gpu in GPUs:\n",
    "    gpu_id = gpu.id\n",
    "    gpu_name = gpu.name\n",
    "    gpuList.append((gpu_id , gpu_name))\n",
    "\n",
    "        \n",
    "e_hardwareinfo = d1.entity('ex:Cell Hardware Info', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_sethardwareinfo = d1.activity('ex:set_hardware_info()')\n",
    "a_platform_uname = d1.activity('ex:platform.uname()')\n",
    "a_cpuinfo = d1.activity('ex:cpuinfo.get_cpu_info()')\n",
    "a_svmemtotal = d1.activity('ex:svmem.total')\n",
    "a_getsize = d1.activity('ex:get_size(svmem.total)')\n",
    "a_GPUtilgetGPU = d1.activity('ex:GPUtil.getGPUs()')\n",
    "d1.wasStartedBy(a_sethardwareinfo, e_hardwareinfo, time=datetime.datetime.now())\n",
    "d1.wasInformedBy(a_sethardwareinfo, a_platform_uname)\n",
    "d1.wasInformedBy(a_sethardwareinfo, a_cpuinfo)\n",
    "d1.wasInformedBy(a_sethardwareinfo, a_svmemtotal)\n",
    "d1.wasInformedBy(a_svmemtotal, a_getsize)\n",
    "d1.wasInformedBy(a_sethardwareinfo, a_GPUtilgetGPU)\n",
    "d1.hadMember(e_notebook, e_hardwareinfo)\n",
    "e_hardwareinfo_data = d1.entity(\n",
    "    'ex:Hardware Info Data',(\n",
    "        ('ex:CPU', cpuinfo.get_cpu_info()['brand_raw']),\n",
    "        ('ex:RAM',  get_size(svmem.total)),\n",
    "        ('ex:Operating System', sysInfo),\n",
    "        ('ex:GPUs', str(gpuList)),\n",
    "))\n",
    "d1.wasGeneratedBy(e_hardwareinfo_data, a_sethardwareinfo)\n",
    "\n",
    "#set_packages\n",
    "cpuInfo_version = !pip list | grep -i py-cpuinfo\n",
    "pytorch_model_summary_version = !pip list | grep -i pytorch-model-summary\n",
    "\n",
    "\n",
    "e_packages = d1.entity('ex:Cell Packages', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setpackages = d1.activity('ex:set_packages()', )\n",
    "a_getVersion = d1.activity('ex:{package_name}.__version__')\n",
    "a_getVersion_py_cpuinfo = d1.activity('ex:!pip list | grep -i py-cpuinfo')\n",
    "a_getVersion_pytorch_model_summary = d1.activity('ex:!pip list | grep -i pytorch-model-summary')\n",
    "d1.wasStartedBy(a_setpackages, e_packages, time=datetime.datetime.now())\n",
    "d1.wasInformedBy(a_setpackages,a_getVersion)\n",
    "d1.wasInformedBy(a_setpackages,a_getVersion_py_cpuinfo)\n",
    "d1.wasInformedBy(a_setpackages,a_getVersion_pytorch_model_summary)\n",
    "d1.hadMember(e_notebook, e_packages)\n",
    "\n",
    "e_packages_data = d1.entity(\n",
    "    'ex:Packages Data',(\n",
    "        ('ex:numpy', np.__version__),\n",
    "    ('ex:ipywidgets', widgets.__version__),\n",
    "    ('ex:torch', torch.__version__),\n",
    "    ('ex:torchvision', torchvision.__version__),\n",
    "    ('ex:pytorch-ignite', pytorch_ignite.__version__),\n",
    "    ('ex:pytorch-lightning',pytorch_lightning.__version__),\n",
    "    ('ex:gputil', GPUtil.__version__),\n",
    "    ('ex:psutil', psutil.__version__),\n",
    "    ('ex:py-cpuinfo', cpuInfo_version[0]),\n",
    "    ('ex:prov', prov.__version__), \n",
    "))\n",
    "d1.wasGeneratedBy(e_packages_data, a_setpackages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe005c4",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6111618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ProvCommunication: (ex:set_data_ingestion(), ex:{dataset}.__len__())>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "\n",
    "\n",
    "dataset = getattr(datasets, 'FakeData')\n",
    "training_dataset = dataset(image_size = (3, 224, 224))\n",
    "testing_dataset = dataset(image_size = (3, 224, 224))\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "dataInfo = training_dataset.__len__\n",
    "#root_location =  str(dataInfo).splitlines()[2]\n",
    "\n",
    "e_dataingestion = d1.entity('ex:Cell Data Ingestion', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "\n",
    "\n",
    "a_setdataingestion = d1.activity('ex:set_data_ingestion()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "d1.wasStartedBy(a_setdataingestion, e_dataingestion)\n",
    "d1.hadMember(e_notebook, e_dataingestion)\n",
    "e_dataingestion_data = d1.entity(\n",
    "    'ex:Data Ingestion Data',(\n",
    "        ('ex:data_format', 'Public dataset'),\n",
    "        ('ex:dataset_id', 'MNIST'),\n",
    "\t\t('ex:feature_classes', 10),\n",
    "\t\t#('ex:training_samples',  training_dataset.__len__()),\n",
    "\t\t#('ex:testing_samples', testing_dataset.__len__()),\n",
    "\t\t#('ex:root_location', root_location),\n",
    "))\n",
    "d1.wasGeneratedBy(e_dataingestion_data, a_setdataingestion)\n",
    "a_splitlines = ('ex:str(dataInfo).splitlines()[2]')\n",
    "a_getlength = d1.activity('ex:{dataset}.__len__()')\n",
    "d1.wasInformedBy(a_setdataingestion, a_getlength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce6e37",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd95cc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ProvCommunication: (ex:set_data_preparation(), ex:str(dataInfo).splitlines()[2])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256), \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(), \n",
    "    #transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # grayscale to RGB\n",
    "])\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "dataInfo = training_dataset.__len__\n",
    "transform_method = str(dataInfo).splitlines()[4:11]\n",
    "\n",
    "e_datapreparation = d1.entity('ex:Cell Data Preparation', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setdatapreparation = d1.activity('ex:set_data_preparation()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "\n",
    "d1.wasStartedBy(a_setdatapreparation, e_datapreparation)\n",
    "d1.hadMember(e_notebook, e_datapreparation)\n",
    "e_datapreparation_data = d1.entity(\n",
    "    'ex:Data Preparation Data',(\n",
    "        ('ex:preprocessing', str(transform_method)),\n",
    "))\n",
    "d1.wasGeneratedBy(e_datapreparation_data, a_setdatapreparation)\n",
    "d1.wasInfluencedBy(e_datapreparation, e_dataingestion_data)\n",
    "d1.wasInformedBy(a_splitlines, a_getlength)\n",
    "d1.wasInformedBy(a_setdatapreparation, a_splitlines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8cf3c4",
   "metadata": {},
   "source": [
    "### Data Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0de5cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ProvCommunication: (ex:set_data_segregation(), ex:{dataset}.__len__())>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "# Set up device.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 128\n",
    "print_every = 1  # batches\n",
    "# Wrap in data loader.\n",
    "dataset = getattr(datasets, 'FakeData')\n",
    "training_dataset = dataset(size = 255, image_size = (3, 224, 224), num_classes = 256, transform = transform)\n",
    "testing_dataset = dataset(size = 255, image_size = (3, 224, 224), num_classes = 256, transform = transform)\n",
    "\n",
    "if use_cuda:\n",
    "    kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
    "else:\n",
    "    kwargs = {}\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "val_loader = None\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "e_datasegregation = d1.entity('ex:Cell Data Segregation', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setdatasegregation = d1.activity('ex:set_data_segregation()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "d1.wasStartedBy(a_setdatasegregation, e_datasegregation)\n",
    "d1.hadMember(e_notebook, e_datasegregation)\n",
    "e_datasegregation_data = d1.entity(\n",
    "    'ex:Data Segregation Data',(\n",
    "        ('ex:training_dataset', str(training_dataset.__len__)),\n",
    "\t\t('ex:testing_dataset', str(testing_dataset.__len__)), \n",
    "))\n",
    "d1.wasGeneratedBy(e_datasegregation_data, a_setdatasegregation)\n",
    "d1.wasInfluencedBy(e_datasegregation, e_datapreparation_data)\n",
    "d1.wasInformedBy(a_setdatasegregation, a_getlength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c539c8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b676c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "# Set up model, loss, optimizer.\n",
    "lr = 0.001\n",
    "model = models.resnet18(pretrained=0)\n",
    "num_classes = 256\n",
    "model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=num_classes, bias=True)\n",
    "model = model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "e_modelparameters = d1.entity('ex:Cell Model Parameters', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setmodelparameters = d1.activity('ex:set_model_parameters()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "d1.wasStartedBy(a_setmodelparameters, e_modelparameters)\n",
    "d1.hadMember(e_notebook, e_modelparameters)\n",
    "e_modelparameters_data = d1.entity(\n",
    "    'ex:Model Parameters Data',(\n",
    "        ('ex:model_name', 'resnet18'),\n",
    "\t\t('ex:save_checkpoint', 0),\n",
    "\t\t('ex:pretrained', 0),\n",
    "\t\t('ex:gpu_enable', 1),\n",
    "\t\t#('ex:modelParameters', str(model)),\n",
    "\t\t('ex:loss_function', 'CrossEntropyLoss'),\n",
    "\t\t('ex:optimizer', 'Adam'),\n",
    "\t\t('ex:optimizer_learning_rate', lr),\n",
    "\t\t('ex:num_classes', 1000),  \n",
    "))\n",
    "d1.wasGeneratedBy(e_modelparameters_data, a_setmodelparameters)\n",
    "d1.wasInfluencedBy(e_modelparameters, e_datasegregation_data)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a6e99",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f882bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 3, batch 1 / 2: loss: 5.696\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "seed_everything(2, workers=True)\n",
    "def set_train_log(log: str, value: str):\n",
    "    log+= value\n",
    "    return log\n",
    "\n",
    "num_epochs = 3\n",
    "epoch_log = \"\"\n",
    "# Set up pytorch-ignite trainer and evaluator.\n",
    "trainer = create_supervised_trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    device=device,\n",
    "    deterministic=True\n",
    ")\n",
    "metrics = {\n",
    "    \"report\": ClassificationReport(),\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"loss\": Loss(loss_func),\n",
    "}\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model, metrics=metrics, device=device\n",
    ")\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=print_every))\n",
    "def log_batch(trainer):\n",
    "    batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1\n",
    "    print(\n",
    "        f\"Epoch {trainer.state.epoch} / {num_epochs}, \"\n",
    "        f\"batch {batch} / {trainer.state.epoch_length}: \"\n",
    "        f\"loss: {trainer.state.output:.3f}\"\n",
    "    )\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_epoch(trainer):\n",
    "    print(f\"Epoch {trainer.state.epoch} / {num_epochs} average results: \")\n",
    "    train_log = set_train_log('', (f\"Epoch {trainer.state.epoch} / {num_epochs} average results: \"))\n",
    "\n",
    "    def log_results(name, metrics, epoch):\n",
    "        print(\n",
    "            f\"{name + ':':6} loss: {metrics['loss']:.3f}, \"\n",
    "            f\"accuracy: {metrics['accuracy']:.3f}\"\n",
    "        )\n",
    "        \n",
    "        log = set_train_log(train_log, (\n",
    "            f\"{name + ':':6} loss: {metrics['loss']:.3f}, \"\n",
    "            f\"accuracy: {metrics['accuracy']:.3f}\"\n",
    "        ))\n",
    "        return log\n",
    "\n",
    "    # Train data.\n",
    "    evaluator.run(train_loader)\n",
    "    log_results(\"train\", evaluator.state.metrics, trainer.state.epoch)\n",
    "    \n",
    "    # Val data.\n",
    "    if val_loader:\n",
    "        evaluator.run(val_loader)\n",
    "        log_results(\"val\", evaluator.state.metrics, trainer.state.epoch)\n",
    "\n",
    "    # Test data.\n",
    "    if test_loader:\n",
    "        evaluator.run(test_loader)\n",
    "        log_results(\"test\", evaluator.state.metrics, trainer.state.epoch)\n",
    "\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    return train_log\n",
    "\n",
    "# Start training.\n",
    "train_log = trainer.run(train_loader, max_epochs=num_epochs)\n",
    "\n",
    "print(train_log)\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "e_training = d1.entity('ex:Cell Training', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_settraining = d1.activity('ex:set_training()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "d1.wasStartedBy(a_settraining, e_training)\n",
    "d1.hadMember(e_notebook, e_training)\n",
    "e_training_data = d1.entity(\n",
    "    'ex:Training Data',(\n",
    "        ('ex:batch_size', batch_size),    \n",
    "\t\t('ex:epochs', num_epochs),\n",
    "\t\t('ex:train_metrics', str(train_log)),\n",
    "\t\t('ex:print_progress', 1),\n",
    "\t\t('ex:seed', 2),\n",
    "))\n",
    "d1.wasGeneratedBy(e_training_data, a_settraining)\n",
    "d1.wasInfluencedBy(e_training, e_modelparameters_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248a8bb",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "e_evaluation = d1.entity('ex:Cell Evaluation', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setevaluation = d1.activity('ex:set_evaluation()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "e_evaluation_data = d1.entity(\n",
    "    'ex:Evaluation Data',(\n",
    "        ('ex:accuracy', evaluator.state.metrics['accuracy']),\n",
    "\t\t('ex:loss', evaluator.state.metrics['loss']),\n",
    "))\n",
    "d1.wasStartedBy(a_setevaluation, e_evaluation)\n",
    "d1.hadMember(e_notebook, e_evaluation)\n",
    "d1.wasGeneratedBy(e_evaluation_data, a_setevaluation)\n",
    "d1.wasInfluencedBy(e_evaluation, e_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be232b",
   "metadata": {},
   "source": [
    "### Generate Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add visualization to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "#get time for filenames\n",
    "timestring = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "ProvenanceNameImage = ('Provenance_ImageClassification_' + timestring + '.png')\n",
    "\n",
    "dot = prov_to_dot(d1, direction='RL')\n",
    "dot.write_png('../GeneratedProvenanceData/'+ProvenanceNameImage)\n",
    "\n",
    "provenanceImage_open = widgets.Button(description = 'Open Image File')\n",
    "display(provenanceImage_open)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    provenanceImage_open.on_click = webbrowser.open('http://localhost:8888/lab/tree/extension/GeneratedProvenanceData/'+ProvenanceNameImage)\n",
    "\n",
    "provenanceImage_open.on_click(on_button_clicked)\n",
    "Image('../GeneratedProvenanceData/'+ProvenanceNameImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5fe25",
   "metadata": {},
   "source": [
    "### Write Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProvenanceName = ('Provenance_ImageClassification_' + timestring + '.json')\n",
    "\n",
    "with open('../GeneratedProvenanceData/'+ProvenanceName, 'w') as prov_file:\n",
    "    prov_file.write(d1.serialize(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3bd230",
   "metadata": {},
   "source": [
    "### Open Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "provenance_open = widgets.Button(description = 'Open Provenance Data File')\n",
    "display(provenance_open)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    provenance_open.on_click = webbrowser.open('http://localhost:8888/lab/tree/extension/GeneratedProvenanceData/'+ProvenanceName)\n",
    "\n",
    "provenance_open.on_click(on_button_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
